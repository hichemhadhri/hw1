{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41f19621",
   "metadata": {},
   "source": [
    "# Homework 1 (HW1)\n",
    "\n",
    "By the end of this homework, we expect you to be able to:\n",
    "\n",
    "- Load data and handle data using pandas;\n",
    "- Navigate the documentation of Python packages by yourself;\n",
    "- Filter and tidy up noisy real-world datasets;\n",
    "- Aggregate your data in different (and hopefully helpful) ways; \n",
    "- Create meaningful visualizations to analyze the data;\n",
    "\n",
    "---\n",
    "\n",
    "## Important Dates\n",
    "\n",
    "- Homework release: Fri 14 Oct 2022\n",
    "- **Homework due**: Sat 29 Oct 2022, 23:59\n",
    "- Grade release: Mon 07 Nov 2022\n",
    "\n",
    "---\n",
    "\n",
    "##  Some rules\n",
    "\n",
    "1. You are allowed to use any built-in Python library that comes with Anaconda. If you want to use an external library, \n",
    "you may do so, but must justify your choice.\n",
    "2. Make sure you use the `data` folder provided in the repository in read-only mode. (Or alternatively, be sure you \n",
    "donâ€™t change any of the files.)\n",
    "3. Be sure to provide a textual description of your thought process, the assumptions you made, the solution you \n",
    "implemented, and explanations for your answers. A notebook that only has code cells will not suffice.\n",
    "4. For questions containing the **/Discuss:/** prefix, answer not with code, but with a textual explanation\n",
    " (**in markdown**).\n",
    "5. Back up any hypotheses and claims with data, since this is an important aspect of the course.\n",
    "6. Please write all your comments in English, and use meaningful variable names in your code. Your repo should have a \n",
    "single notebook (plus the required data files) in the *master/main* branch. If there are multiple notebooks present, \n",
    "we will **not grade** anything.\n",
    "7. We will **not run your notebook for you**! Rather, we will grade it as is, which means that only the results \n",
    "contained in your evaluated code cells will be considered, and we will not see the results in unevaluated code cells. \n",
    "Thus, be sure to hand in a **fully-run and evaluated notebook**. In order to check whether everything looks as intended,\n",
    " you can check the rendered notebook on the GitHub website once you have pushed your solution there.\n",
    "8. In continuation to the previous point, interactive plots, such as those generated using `plotly`, should be \n",
    "**strictly avoided**!\n",
    "9. Make sure to print results or dataframes that confirm you have properly addressed the task.\n",
    "\n",
    "---\n",
    "\n",
    "In this homework, we will analyze data from A/B tests of headlines conducted by Upworthy from January 2013 to April 2015 to study whether the language used in the headline determines the number of people that will read the associated news piece. The homework contains four tasks: in task 1, we will process the data; in task 2, we will extract meaningful signals from the data; in task 3, we will test whether the language of headlines impacts their success; and in task 4, we will explore the heterogeneity of this effects (e.g., does it vary through time?).\n",
    "\n",
    "\n",
    "### **What is an A/B test?** \n",
    "A/B tests are experiments that compare two scenarios (e.g., scenario A vs. scenario B). \n",
    "They test subjects' responses to each of the variants to determine which is more effective ([read more about A/B tests on Wikipedia](https://en.wikipedia.org/wiki/A/B_testing)). \n",
    "A/B tests allow us to draw conclusions about the different scenarios by randomizing exposure to them, e.g., one could flip a coin and assign a user to scenario A if it lands heads and to B if it lands tails. \n",
    "Since exposure is randomized, we can be confident that the scenarios are the sole explanation for statistically significant differences in subjects' responses (if they exist). \n",
    "In theory, A/B testing refers to an experiment that compares two scenarios; however, in practice, the term is also used when we compare multiple scenarios (e.g., A vs. B vs. C), although the more precise terminology would be to call such an experiment a \"multinomial test.\"\n",
    "\n",
    "### **How were A/B tests used by Upworthy?** \n",
    "Upworthy used A/B testing to increase news readership, conducting experiments for each published news piece. \n",
    "In each experiment, they created multiple \"packages\" of stimuli, varying headlines, images, excerpts, and ledes for the same news piece. \n",
    "Different \"packages\" were shown on their (now defunct) website to engage users with the news pieces they produced. Upworthy found \"the best\" package by conducting A/B tests, showing different packages to different users, and measuring how often users clicked on each version. \n",
    "Below, we show three \"packages\" used by Upworthy in an experiment, each with a different headline for the same news piece. \n",
    "Upworthy randomized users that visited their website saw one of the three versions of the headline below. Then, they measured the percentage of times users in each scenario clicked to read the news. \n",
    "The headline with the highest percentage of clicks per view (click through rate) was then declared the \"winner\" and became the default for all visitors.\n",
    "\n",
    "![Example A/B test](example.png)\n",
    "\n",
    " ### **Where does this data come from?** \n",
    " \n",
    " From a paper [1].\n",
    "\n",
    "[1] Matias, J.N., Munger, K., Le Quere, M.A. et al. The Upworthy Research Archive, a time series of 32,487 experiments in U.S. media. Sci Data 8, 195 (2021). https://doi.org/10.1038/s41597-021-00934-7\n",
    "\n",
    "### **Where can I find this data?**  \n",
    "\n",
    "You can find it in the `/data/` folder.\n",
    "\n",
    "### **Terminology**\n",
    "\n",
    "- **News piece:** A news article. In the dataset considered, these were all produced by Upworthy.\n",
    "- **Package:** The set of visual stimuli inviting the user to read an article. The figure above shows a package with a headline and an image. At times, there was an excerpt of the article also shown in the package and/or the lede, i.e., [\"the introductory section of a news story that is intended to entice the reader to read the full story.\"](https://www.merriam-webster.com/words-at-play/bury-the-lede-versus-lead#:~:text=In%20journalism%2C%20the%20lede%20refers,machines%20began%20disappearing%20from%20newsrooms.)\n",
    "- **Experiment:** Each experiment is an A/B test (or multinomial test, to be more precise) comparing how users reacted to different \"packages.\" Experiments measured two things: 1) how many users were shown each package; and 2) how many individuals clicked each package.\n",
    "\n",
    "### **Data description**\n",
    "\n",
    "| Column name          | Description                                                                                                                                                                                       |   |   |   |\n",
    "|----------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---|---|---|\n",
    "| created_at           | Time the package was created (timezone unknown)                                                                                                                                                   |   |   |   |\n",
    "| test_week            | Week the package was created, a variable constructed by the archive creators for stratified random sampling                                                                                       |   |   |   |\n",
    "| clickability_test_id | The test ID. Viewers were randomly assigned to packages with the same test ID                                                                                                                     |   |   |   |\n",
    "| impressions          | The number of viewers who were assigned to this package. The total number of participants for a given test is the sum of impressions for all packages that share the same clickability_test_id    |   |   |   |\n",
    "| headline             | The headline being tested                                                                                                                                                                         |   |   |   |\n",
    "| eyecatcher_id        | Image ID. Image files are not available. Packages that shared the same image have the same eyecatcher_id                                                                                          |   |   |   |\n",
    "| clicks               | The number of viewers (impressions) that clicked on the package. The clickrate for a given package is the number of clicks divided by the number of impressions                                   |   |   |   |\n",
    "| excerpt              | Article excerpt                                                                                                                                                                                   |   |   |   |\n",
    "| lede                 | The opening sentence or paragraph of the story                                                                                                                                                    |   |   |   |\n",
    "| slug                 | Internal name for the web address                                                                                                                                                                 |   |   |   |\n",
    "| share_text           | Summary for display on social media when the article is shared. This was not shown in tests, since tests were conducted on the Upworthy website                                                   |   |   |   |\n",
    "| square               | When used, part of the same social media sharing suggestion as the share text                                                                                                                     |   |   |   |\n",
    "| significance         | NOT an estimate of statistical significance; a complex, inconsistent calculation that compared the clicks on a package to the clicks on all previous packages that were fielded on the same pages |   |   |   |\n",
    "| first_place          | Along with significance, shown to editors to guide decisions about what test to choose                                                                                                            |   |   |   |\n",
    "| winner               | Whether a package was selected by editors to be used on the Upworthy site after the test                                                                                                          |   |   |   |\n",
    "| updated_at           | The last time the package was updated in the Upworthy system                                                                                                                                      |   |   |   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b640175",
   "metadata": {},
   "source": [
    "## Task 1: Getting familiar with the data\n",
    "\n",
    "Your first task is to conduct initial analyses to understand the data and process it in a way that will allow us to more easily answer our key question: *how does the language of a headline determine its success?*\n",
    "\n",
    "1.1 Load the data into memory using pandas and print the first lines to get a sense of it.\n",
    "\n",
    "1.2 Each experiment comparing different versions of the same news piece (\"packages\") has a unique identifier (`clickability_test_id` column). \n",
    "Calculate how many different experiments were conducted in this dataset and, on average, how many packages were considered per experiment. \n",
    "Last, plot the distribution of packages per experiment with a visualization of your choice.\n",
    "\n",
    "1.3 A common way to measure success in online A/B tests is what is called \"the clickthrough rate.\"\n",
    "Given that often A/B tests are created to find what engages users (here, \"packages\" of headlines, images, etc), we would expect that a \"good\" package makes people click often. \n",
    "Create a column named `ctr` by dividing the number of clicks a package received (`clicks` column) by the number of impressions it received (`impressions` column).\n",
    "\n",
    "1.4 Packages varied any combination of the headline (`headline` column), the excerpt (`excerpt`), the first sentence of the article (`lede`), and the image that illustrates the news piece (`eyecatcher_id`, a hash per image). \n",
    "But we want to isolate the effect of the headline on the clickthrough rate. To do that, create a new dataframe where you filter all experiments where only one headline is present. \n",
    "Print the length of this new dataframe and how many experiments were discarded in the filtering process.\n",
    "\n",
    "1.5 For comparison, repeat the procedure described in **T1.4** with the `eyecatcher_id` column, i.e., create a dataframe considering only experiments that vary the image. \n",
    "Again, print the length of this new dataframe and how many experiments were discarded in the filtering process.\n",
    "\n",
    "1.6 **Discuss:** Considering the answers to questions **T1.4** and **T1.4**, what can we say about the different versions of the news tested by Upworthy?\n",
    "\n",
    "1.7 For our subsequent analysis, we want to compare the causal effect of headlines on the success of a news piece. \n",
    "For that, we can compare pairs of packages with the same `eyecatcher_id`, `lede`, and `excerpt`, but different `headlines`.\n",
    "Note that this means that if an experiment considered 5 different headlines and did not vary any other stimulus, we would have 5C2 (i.e., 5 choose 2, 10) pairs to consider.\n",
    "Create a dataset where:\n",
    "- each row corresponds to a pair of packages with different `headline` but the same `eyecatcher_id`, `lede`, and `excerpt`. \n",
    "- there are columns containing the headlines of each of the news versions (`headline1`, `headline2`) and the clickthrough rate of each of the news versions (`ctr1`, `ctr2`). \n",
    "- the columns `headline1` and `ctr1` contain the data associated with the news version with the highest clickthrough rate. Print the first columns of your newly created dataframe, as well as its length.\n",
    "-  the columns where the two news pieces had exactly the same clickthrough rate should be filtered out (this is for simplicity's sake).\n",
    "-  the column `date_created` contains the date when the news version with the highest clickthrough rate was created.\n",
    "\n",
    "1.8 To get a sense of the impact of headline change, measure the average difference per pair between the most clicked-through (`ctr1`) and the least clicked-through headline (`ctr2`), as well as the average clickthrough rate for the least clicked through headline (`ctr2`). \n",
    "\n",
    "1.9 **Discuss:** Considering your answer to **T1.8**, and assuming the average differences in clickthrough rates between pairs are statistically significant, do you think that headlines are impactful in the news business? Justify with the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f25d88",
   "metadata": {},
   "source": [
    "1.1 Load the data into memory using pandas and print the first lines to get a sense of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a9f611",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_ind, ttest_rel\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "data_folder = \"./Data/\"\n",
    "\n",
    "packages = pd.read_csv(\n",
    "    data_folder + \"upworthy.csv.gz\",\n",
    "    parse_dates=[\"created_at\", \"updated_at\"],\n",
    "    na_values=[\"Nan\"],\n",
    ")\n",
    "packages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0b65ac",
   "metadata": {},
   "source": [
    "1.2 Each experiment comparing different versions of the same news piece (\"packages\") has a unique identifier (clickability_test_id column). Calculate how many different experiments were conducted in this dataset and, on average, how many packages were considered per experiment. Last, plot the distribution of packages per experiment with a visualization of your choice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1db9282",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_by_id = packages.groupby(\"clickability_test_id\")\n",
    "count_by_experiment = packages.groupby(\"clickability_test_id\").count()[\"created_at\"]\n",
    "\n",
    "nb_experiment = count_by_experiment.count()\n",
    "mean_nb_experiment = count_by_experiment.mean()\n",
    "variance_nb_experiment = count_by_experiment.var()\n",
    "print(\n",
    "    \"The total number expremients is %d with on averange %0.2f packages considered per experiment and a variance of %0.1f packages \"\n",
    "    % (nb_experiment, mean_nb_experiment, variance_nb_experiment)\n",
    ")\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "fig.suptitle(\"Number of packages per expermient\")\n",
    "ax1.hist(count_by_experiment.values, bins=100)\n",
    "\n",
    "### TODO Should we keep yscale = \"log\" ?\n",
    "\n",
    "ax1.set(xlabel=\"Number of packages\", ylabel=\"Number of expermiments\", yscale=\"log\")\n",
    "ax2.boxplot(count_by_experiment)\n",
    "ax2 = ax2.set(xticks=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af06a12c",
   "metadata": {},
   "source": [
    "1.3 A common way to measure success in online A/B tests is what is called \"the clickthrough rate.\" Given that often A/B tests are created to find what engages users (here, \"packages\" of headlines, images, etc), we would expect that a \"good\" package makes people click often. Create a column named ctr by dividing the number of clicks a package received (clicks column) by the number of impressions it received (impressions column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d1979d",
   "metadata": {},
   "outputs": [],
   "source": [
    "packages[\"ctr\"] = packages.apply(lambda row: row[\"clicks\"] / row[\"impressions\"], axis=1)\n",
    "packages.sort_values(by=\"ctr\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdd7798",
   "metadata": {},
   "source": [
    "1.4 Packages varied any combination of the headline (headline column), the excerpt (excerpt), the first sentence of the article (lede), and the image that illustrates the news piece (eyecatcher_id, a hash per image). But we want to isolate the effect of the headline on the clickthrough rate. To do that, create a new dataframe where you filter all experiments where only one headline is present. Print the length of this new dataframe and how many experiments were discarded in the filtering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f4e72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Packages where there is more than one headline\n",
    "not_unique_headline_df = packages.groupby(by=[\"clickability_test_id\"]).filter(\n",
    "    lambda x: x[\"headline\"].nunique() != 1\n",
    ")\n",
    "\n",
    "### Number of expermient where there is more than one headline\n",
    "not_unique_headline_exp_count = (\n",
    "    not_unique_headline_df.groupby(\"clickability_test_id\").count()[\"created_at\"].count()\n",
    ")\n",
    "print(\n",
    "    \"The new dataframe length is %d and %d experiments from %d were dropped \"\n",
    "    % (\n",
    "        len(not_unique_headline_df),\n",
    "        nb_experiment - not_unique_headline_exp_count,\n",
    "        nb_experiment,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5848de72",
   "metadata": {},
   "source": [
    "1.5 For comparison, repeat the procedure described in T1.4 with the eyecatcher_id column, i.e., create a dataframe considering only experiments that vary the image. Again, print the length of this new dataframe and how many experiments were discarded in the filtering process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89444d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Packages where there is more than one image\n",
    "not_unique_image_df = packages.groupby(by=[\"clickability_test_id\"]).filter(\n",
    "    lambda x: x[\"eyecatcher_id\"].nunique() != 1\n",
    ")\n",
    "\n",
    "### Number of expermient where there is more than one image\n",
    "not_unique_image_exp_count = (\n",
    "    not_unique_image_df.groupby(\"clickability_test_id\").count()[\"created_at\"].count()\n",
    ")\n",
    "print(\n",
    "    \"The new dataframe length is %d and %d experiments from %d were dropped \"\n",
    "    % (\n",
    "        len(not_unique_image_df),\n",
    "        nb_experiment - not_unique_image_exp_count,\n",
    "        nb_experiment,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54920f75",
   "metadata": {},
   "source": [
    "1.6 **Discuss:** Considering the answers to questions **T1.4** and **T1.4**, what can we say about the different versions of the news tested by Upworthy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa893da5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6070919e",
   "metadata": {},
   "source": [
    "1.7 For our subsequent analysis, we want to compare the causal effect of headlines on the success of a news piece. \n",
    "For that, we can compare pairs of packages with the same `eyecatcher_id`, `lede`, and `excerpt`, but different `headlines`.\n",
    "Note that this means that if an experiment considered 5 different headlines and did not vary any other stimulus, we would have 5C2 (i.e., 5 choose 2, 10) pairs to consider.\n",
    "Create a dataset where:\n",
    "- each row corresponds to a pair of packages with different `headline` but the same `eyecatcher_id`, `lede`, and `excerpt`. \n",
    "- there are columns containing the headlines of each of the news versions (`headline1`, `headline2`) and the clickthrough rate of each of the news versions (`ctr1`, `ctr2`). \n",
    "- the columns `headline1` and `ctr1` contain the data associated with the news version with the highest clickthrough rate. Print the first columns of your newly created dataframe, as well as its length.\n",
    "-  the columns where the two news pieces had exactly the same clickthrough rate should be filtered out (this is for simplicity's sake).\n",
    "-  the column `date_created` contains the date when the news version with the highest clickthrough rate was created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a553006",
   "metadata": {},
   "outputs": [],
   "source": [
    "not_unique_headline_df.groupby(by=[\"clickability_test_id\"]).get_group(\n",
    "    \"516492475f77be0002011c74\"\n",
    ")[\"excerpt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab78725",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_unique_head_df = not_unique_headline_df.groupby(\n",
    "    by=[\"clickability_test_id\"]\n",
    ").filter(\n",
    "    lambda x: x[\"eyecatcher_id\"].nunique() == 1\n",
    "    and x[\"lede\"].nunique() == 1\n",
    "    and x[\"excerpt\"].nunique() == 1\n",
    "    and x[\"headline\"].nunique() == len(x[\"headline\"])\n",
    ")\n",
    "\n",
    "filtered_unique_head_df = filtered_unique_head_df.dropna(\n",
    "    subset=[\"excerpt\", \"eyecatcher_id\", \"lede\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4cad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = not_unique_headline_df.merge(\n",
    "    not_unique_headline_df,\n",
    "    on=[\"clickability_test_id\", \"eyecatcher_id\", \"lede\", \"excerpt\"],\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "df.groupby([\"clickability_test_id\", \"eyecatcher_id\", \"lede\", \"excerpt\"]).agg(list).index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d1129",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Join the table with itself using considering 'clickability_test_id', 'eyecatcher_id', 'lede','excerpt' for joining\n",
    "comparaison = filtered_unique_head_df.merge(\n",
    "    not_unique_headline_df,\n",
    "    on=[\"clickability_test_id\", \"eyecatcher_id\", \"lede\", \"excerpt\"],\n",
    "    how=\"inner\",\n",
    ")\n",
    "\n",
    "\n",
    "### Filtering out rows where clickthrough rate is identical\n",
    "comparaison = comparaison.drop(\n",
    "    comparaison[comparaison[\"ctr_x\"] == comparaison[\"ctr_y\"]].index\n",
    ")\n",
    "\n",
    "### New dataframe\n",
    "comparaison_final = pd.DataFrame()\n",
    "### Retreiving ids for identification\n",
    "comparaison_final[\"clickability_test_id\"] = comparaison[\"clickability_test_id\"]\n",
    "### Selecting headline1, headline2 , ctr1, ctr2, date_created based on ctr_comapraison criteria\n",
    "comparaison_final[\"headline1\"] = comparaison.apply(\n",
    "    lambda row: row[\"headline_x\"] if row[\"ctr_x\"] > row[\"ctr_y\"] else row[\"headline_y\"],\n",
    "    axis=1,\n",
    ")\n",
    "comparaison_final[\"headline2\"] = comparaison.apply(\n",
    "    lambda row: row[\"headline_x\"] if row[\"ctr_x\"] < row[\"ctr_y\"] else row[\"headline_y\"],\n",
    "    axis=1,\n",
    ")\n",
    "comparaison_final[\"ctr1\"] = comparaison.apply(\n",
    "    lambda row: row[\"ctr_x\"] if row[\"ctr_x\"] > row[\"ctr_y\"] else row[\"ctr_y\"], axis=1\n",
    ")\n",
    "comparaison_final[\"ctr2\"] = comparaison.apply(\n",
    "    lambda row: row[\"ctr_x\"] if row[\"ctr_x\"] < row[\"ctr_y\"] else row[\"ctr_y\"], axis=1\n",
    ")\n",
    "comparaison_final[\"date_created\"] = comparaison.apply(\n",
    "    lambda row: row[\"created_at_x\"]\n",
    "    if row[\"ctr_x\"] > row[\"ctr_y\"]\n",
    "    else row[\"created_at_y\"],\n",
    "    axis=1,\n",
    ")\n",
    "### Dropping duplicates\n",
    "comparaison_final = comparaison_final.drop_duplicates()\n",
    "\n",
    "print(\"The DataSetLength is equal to %d \" % len(comparaison_final))\n",
    "comparaison_final.sort_values(by=[\"ctr1\", \"ctr2\"], ascending=[False, False]).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37c1a50",
   "metadata": {},
   "source": [
    "1.8 To get a sense of the impact of headline change, measure the average difference per pair between the most clicked-through (`ctr1`) and the least clicked-through headline (`ctr2`), as well as the average clickthrough rate for the least clicked through headline (`ctr2`). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a042338",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparaison_final[\"delta_ctr\"] = comparaison_final.apply(\n",
    "    lambda row: row[\"ctr1\"] - row[\"ctr2\"], axis=1\n",
    ")\n",
    "\n",
    "print(\n",
    "    \"The average difference per pair between the most clicked-through and the least clicked-through headline is %f and the average clickthrough rate for the least clicked through headline is %f\"\n",
    "    % (comparaison_final[\"delta_ctr\"].mean(), comparaison_final[\"ctr2\"].mean())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00795a15",
   "metadata": {},
   "source": [
    "## Task 2: Extracting signals from the data\n",
    "\n",
    "Your second task is to extract meaningful signals from the data. \n",
    "We start this task from the dataset obtained in **T1.7**. \n",
    "Recall that we have one A/B test per row with the clickthrough rate of two news pieces that differ only in their headline. \n",
    "We refer to the version with the higher clickthrough rate as the \"winner\" and the version with the lower as the \"loser.\" \n",
    "(Note that this is not the same as the column `winner` in the original data, which captures a similar concept but considering the original experiments, where multiple comparisons were made!)\n",
    " \n",
    "2.1 Using the function provided below, count the number of words in each headline, creating columns `numwords1` and `numwords2` corresponding to the number of words in the winner and loser headlines.\n",
    "\n",
    "2.2 Using the dictionary of pronouns provided below, create indicator variables corresponding to each set of pronouns (e.g., first-person singular may yield columns `first_person_singular1` and `first_person_singular2` for the headlines in each A/B test). \n",
    "Each indicator variable in the dataframe should equal 1 if the corresponding headline uses the corresponding type of pronoun and 0 otherwise. \n",
    "Your code should be agnostic to lower/upper case.\n",
    "\n",
    "2.3 One easy way to classify sentiment is simply to match negative or positive words. \n",
    "Use the linked lists of words ([positive][1], [negative][2]) to obtain \"positive sentiment\" and \"negative sentiment\" scores for each headline. Create columns `positive1`/`positive2` and `negative1`/`negative2` containing indicator variables for positive and negative sentiment, i.e., A headline has a \"positive sentiment\" (or negative) score equal 1 if it contains at least one positive (or negative) sentiment word on the list. Otherwise, its \"positive sentiment\" (or negative) score equals 0.\n",
    "    \n",
    "[1]: https://ptrckprry.com/course/ssd/data/positive-words.txt\n",
    "[2]: https://ptrckprry.com/course/ssd/data/negative-words.txt\n",
    "\n",
    "--- \n",
    "\n",
    "**Comments**\n",
    "\n",
    "- For **T2.3**, beware of encodings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9cba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1 (provided code)\n",
    "def count_words_simple(x):\n",
    "    return len(x.split(\" \"))\n",
    "\n",
    "\n",
    "str_test = \"How many words are here?\"\n",
    "print(str_test, count_words_simple(str_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f678ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2 (provided code)\n",
    "feature_wordsets = dict(\n",
    "    [\n",
    "        # https://en.wikipedia.org/wiki/English_personal_pronouns\n",
    "        (\n",
    "            \"first_person_singular\",\n",
    "            [\n",
    "                \"i\",\n",
    "                \"me\",\n",
    "                \"my\",\n",
    "                \"mine\",\n",
    "                \"myself\",\n",
    "                \"i'd\",\n",
    "                \"i'll\",\n",
    "                \"i'm\",\n",
    "                \"i've\",\n",
    "                \"id\",\n",
    "                \"im\",\n",
    "                \"ive\",\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            \"first_person_plural\",\n",
    "            [\n",
    "                \"we\",\n",
    "                \"us\",\n",
    "                \"our\",\n",
    "                \"ours\",\n",
    "                \"ourselves\",\n",
    "                \"we'd\",\n",
    "                \"we'll\",\n",
    "                \"we're\",\n",
    "                \"we've\",\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            \"second_person\",\n",
    "            [\n",
    "                \"you\",\n",
    "                \"your\",\n",
    "                \"yours\",\n",
    "                \"yourself\",\n",
    "                \"ya\",\n",
    "                \"you'd\",\n",
    "                \"you'll\",\n",
    "                \"you're\",\n",
    "                \"you've\",\n",
    "                \"youll\",\n",
    "                \"youre\",\n",
    "                \"youve\",\n",
    "                \"yourselves\",\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            \"third_person_singular\",\n",
    "            [\n",
    "                \"he\",\n",
    "                \"him\",\n",
    "                \"his\",\n",
    "                \"himself\",\n",
    "                \"he'd\",\n",
    "                \"he's\",\n",
    "                \"hes\",\n",
    "                \"she\",\n",
    "                \"her\",\n",
    "                \"hers\",\n",
    "                \"herself\",\n",
    "                \"she'll\",\n",
    "                \"she's\",\n",
    "                \"shes\",\n",
    "                \"it\",\n",
    "                \"its\",\n",
    "                \"itself\",\n",
    "                \"themself\",\n",
    "            ],\n",
    "        ),\n",
    "        (\n",
    "            \"third_person_plural\",\n",
    "            [\n",
    "                \"they\",\n",
    "                \"them\",\n",
    "                \"their\",\n",
    "                \"theirs\",\n",
    "                \"themselves\",\n",
    "                \"they'd\",\n",
    "                \"they'll\",\n",
    "                \"they've\",\n",
    "                \"theyll\",\n",
    "                \"theyve\",\n",
    "            ],\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c5d692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5691bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparaison_final.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db63064",
   "metadata": {},
   "source": [
    "2.1 Using the function provided below, count the number of words in each headline, creating columns `numwords1` and `numwords2` corresponding to the number of words in the winner and loser headlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d27db",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparaison_final[\"numwords1\"] = comparaison_final.apply(\n",
    "    lambda row: count_words_simple(row[\"headline1\"]), axis=1\n",
    ")\n",
    "comparaison_final[\"numwords2\"] = comparaison_final.apply(\n",
    "    lambda row: count_words_simple(row[\"headline2\"]), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1010c4",
   "metadata": {},
   "source": [
    "2.2 Using the dictionary of pronouns provided below, create indicator variables corresponding to each set of pronouns (e.g., first-person singular may yield columns `first_person_singular1` and `first_person_singular2` for the headlines in each A/B test). \n",
    "Each indicator variable in the dataframe should equal 1 if the corresponding headline uses the corresponding type of pronoun and 0 otherwise. \n",
    "Your code should be agnostic to lower/upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea31a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in feature_wordsets:\n",
    "    comparaison_final[key + \"1\"] = comparaison_final.apply(\n",
    "        lambda row: 0\n",
    "        if len(set(row[\"headline1\"].lower().split(\" \")) & set(feature_wordsets[key]))\n",
    "        == 0\n",
    "        else 1,\n",
    "        axis=1,\n",
    "    )\n",
    "    comparaison_final[key + \"2\"] = comparaison_final.apply(\n",
    "        lambda row: 0\n",
    "        if len(set(row[\"headline2\"].lower().split(\" \")) & set(feature_wordsets[key]))\n",
    "        == 0\n",
    "        else 1,\n",
    "        axis=1,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95fdcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparaison_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511982eb",
   "metadata": {},
   "source": [
    "2.3 One easy way to classify sentiment is simply to match negative or positive words. \n",
    "Use the linked lists of words ([positive][1], [negative][2]) to obtain \"positive sentiment\" and \"negative sentiment\" scores for each headline. Create columns `positive1`/`positive2` and `negative1`/`negative2` containing indicator variables for positive and negative sentiment, i.e., A headline has a \"positive sentiment\" (or negative) score equal 1 if it contains at least one positive (or negative) sentiment word on the list. Otherwise, its \"positive sentiment\" (or negative) score equals 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec30ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read positive words file\n",
    "with open(data_folder + \"positive-words.txt\") as p:\n",
    "    pos = p.readlines()[35:]\n",
    "\n",
    "pos = [s.strip() for s in pos]\n",
    "\n",
    "# read negative words file\n",
    "with open(data_folder + \"positive-words.txt\") as n:\n",
    "    neg = n.readlines()[35:]\n",
    "\n",
    "neg = [s.strip() for s in neg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9f3d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparaison_final[\"positive1\"] = comparaison_final.apply(\n",
    "    lambda row: 0\n",
    "    if len(set(row[\"headline1\"].lower().split(\" \")) & set(pos)) == 0\n",
    "    else 1,\n",
    "    axis=1,\n",
    ")\n",
    "comparaison_final[\"positive2\"] = comparaison_final.apply(\n",
    "    lambda row: 0\n",
    "    if len(set(row[\"headline2\"].lower().split(\" \")) & set(pos)) == 0\n",
    "    else 1,\n",
    "    axis=1,\n",
    ")\n",
    "comparaison_final[\"negative1\"] = comparaison_final.apply(\n",
    "    lambda row: 0\n",
    "    if len(set(row[\"headline1\"].lower().split(\" \")) & set(neg)) == 0\n",
    "    else 1,\n",
    "    axis=1,\n",
    ")\n",
    "comparaison_final[\"negative2\"] = comparaison_final.apply(\n",
    "    lambda row: 0\n",
    "    if len(set(row[\"headline2\"].lower().split(\" \")) & set(neg)) == 0\n",
    "    else 1,\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572f0b46",
   "metadata": {},
   "source": [
    "## Task 3: Estimating the effect of language on headline success\n",
    "\n",
    "Your third task revolves around the question *how does language impact headlines' success?*\n",
    "\n",
    "3.1 First, we examine whether the winner headlines have more or fewer words than the loser headline. Conduct an independent sample t-test and paired t-test (see [scipy.stats](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html#scipy.stats.ttest_ind); for the independent sample t-test, assume equal variance). Also, calculate and print the mean difference between the number of words in the winner and the loser headlines.\n",
    "\n",
    "3.2 **Discuss:** Are longer headlines more successful? Justify.\n",
    "\n",
    "3.3 The [t-statistic](https://en.wikipedia.org/wiki/T-statistic) is the ratio of the departure of the estimated value of a parameter from its hypothesized value to its standard error. In a t-test, the higher the t-statistic, the more confidently we can reject the null hypothesis. Use `numpy.random` to create four samples, each of size 30:\n",
    "- $X \\sim Uniform(0,1)$\n",
    "- $Y \\sim Uniform(0,1)$\n",
    "- $Z = X/2 + Y/2 + 0.1$\n",
    "- $K = Y + 0.1$\n",
    "    \n",
    "3.4 **Discuss:** What are the expected values and the variance of $X$, $Y$, $Z$, and $K$? (You don't need to justify them!)\n",
    "\n",
    "3.5 Run the following simulation 10000 times, storing the $p$-values for the tests at each run:\n",
    "- Sample new values  for $X$, $Y$, $Z$ and $K$ ($n=30$ each). \n",
    "- Run independent sample t-test (assuming equal variance) and paired t-test comparing $X$ and $Z$.\n",
    "-  Run independent sample t-test (assuming equal variance) and paired t-test comparing $X$ and $K$.\n",
    "\n",
    "3.6 Recall that the power of a binary hypothesis test is the probability that the test correctly rejects the null hypothesis when the alternative hypothesis is true. Using the p-values and assuming that we reject the null hypothesis if $p < 0.05$, calculate the statistical power of:\n",
    "- The independent sample t-test comparing $X$ and $Z$.\n",
    "- The paired t-test comparing $X$ and $Z$.\n",
    "- The independent sample t-test comparing $X$ and $K$.\n",
    "- The paired t-test comparing $X$ and $K$.\n",
    "    \n",
    "3.7 **Discuss:** When are paired t-tests helpful? Justify.\n",
    "\n",
    "3.8 With a bootstrapping approach (implemented by yourself, you should not use existing bootstrapping functions), estimate the average difference and 95% confidence intervals for:\n",
    "- the mean ratio between the number of words in the winner headline and the loser headline (i.e., the number of words in the winner headline divided by the number of words in the loser headlines).\n",
    "- the difference in usage of positive words between winner and loser headlines.\n",
    "- the difference in usage of negative words between winner and loser headlines.\n",
    "- The difference in usage of each type of pronoun between winner and loser headlines.\n",
    "\n",
    "3.9 **Discuss:** According to the results obtained in **T3.8**, what headlines grab people's attention the most? Justify your answer.\n",
    "    \n",
    "---\n",
    "**Comments:**\n",
    "\n",
    "- Paired t-test formula: $t = \\frac{\\overline{x}_{\\mathrm{diff}}}{s_{\\mathrm{diff}} / \\sqrt n }$ where:\n",
    "    - $\\overline{x}_{\\mathrm{diff}}$ is the sample difference between the means of the matched sample; and\n",
    "    - $s_{\\mathrm{diff}}$ is the sample variance of the matched sample; and\n",
    "    - $n$ is the number of matched samples.\n",
    "    \n",
    "- Independent samples t-test formula: $t = \\frac{\\overline{x}_{1} - \\overline{x}_{2}}{\\sqrt{\\frac{s_{1}^{2}}{n_{1}} + \\frac{s_{2}^{2}}{n_{2}}}}$ where:\n",
    "    - $\\overline{x}_{\\mathrm{1}}$ is the sample mean of the first group; and\n",
    "    - $s_{\\mathrm{1}}$ is the sample variance of the first group; and\n",
    "    - $n_1$ is the number of samples in the first group;\n",
    "    \n",
    "     \n",
    "- t-tests are valid for samples of non-normal distribution for large enough samples (a rule of thumb used is: n$\\geq$30)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f96717",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparaison_final.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966c66d6",
   "metadata": {},
   "source": [
    "3.1 First, we examine whether the winner headlines have more or fewer words than the loser headline. Conduct an independent sample t-test and paired t-test (see [scipy.stats](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_ind.html#scipy.stats.ttest_ind); for the independent sample t-test, assume equal variance). Also, calculate and print the mean difference between the number of words in the winner and the loser headlines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_t_test = ttest_ind(\n",
    "    comparaison_final[\"numwords1\"], comparaison_final[\"numwords2\"], equal_var=True\n",
    ")\n",
    "paired_t_test = ttest_rel(\n",
    "    comparaison_final[\"numwords1\"], comparaison_final[\"numwords2\"]\n",
    ")\n",
    "mean_diff = (comparaison_final[\"numwords1\"] - comparaison_final[\"numwords2\"]).mean()\n",
    "\n",
    "print(\n",
    "    \"independent t-test results {ind} , paired t-test results {pair} and the mean difference is {m}\".format(\n",
    "        ind=ind_t_test, pair=paired_t_test, m=mean_diff\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b7d73e",
   "metadata": {},
   "source": [
    "We can safely reject that that the winner and the loser have the same average for both tests since the p-value is less than 0.05\n",
    "In addition, the mean difference is small so we cannot conclude that longer headlines are more successfull\n",
    "**chouf maa rojla fi hedhi**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d8cf8b",
   "metadata": {},
   "source": [
    "3.3 The [t-statistic](https://en.wikipedia.org/wiki/T-statistic) is the ratio of the departure of the estimated value of a parameter from its hypothesized value to its standard error. In a t-test, the higher the t-statistic, the more confidently we can reject the null hypothesis. Use `numpy.random` to create four samples, each of size 30:\n",
    "- $X \\sim Uniform(0,1)$\n",
    "- $Y \\sim Uniform(0,1)$\n",
    "- $Z = X/2 + Y/2 + 0.1$\n",
    "- $K = Y + 0.1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63bb356",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(low=0, high=1, size=30)\n",
    "y = np.random.uniform(low=0, high=1, size=30)\n",
    "z = x / 2 + y / 2 + 0.1\n",
    "k = y + 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f78e634",
   "metadata": {},
   "source": [
    "3.4 **Discuss:** What are the expected values and the variance of $X$, $Y$, $Z$, and $K$? (You don't need to justify them!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3e4c6a",
   "metadata": {},
   "source": [
    "var x , y = 1/12 , var z = 1/24 , var k = 1/12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d273bf5",
   "metadata": {},
   "source": [
    "3.5 Run the following simulation 10000 times, storing the $p$-values for the tests at each run:\n",
    "- Sample new values  for $X$, $Y$, $Z$ and $K$ ($n=30$ each). \n",
    "- Run independent sample t-test (assuming equal variance) and paired t-test comparing $X$ and $Z$.\n",
    "-  Run independent sample t-test (assuming equal variance) and paired t-test comparing $X$ and $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c1947",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iters = 10000\n",
    "p_values_ind_x_z = []\n",
    "p_values_paired_x_z = []\n",
    "p_values_ind_x_k = []\n",
    "p_values_paired_x_k = []\n",
    "for i in range(n_iters):\n",
    "    x = np.random.uniform(low=0, high=1, size=30)\n",
    "    y = np.random.uniform(low=0, high=1, size=30)\n",
    "    z = x / 2 + y / 2 + 0.1\n",
    "    k = y + 0.1\n",
    "    p_values_ind_x_z.append(ttest_ind(x, z, equal_var=True).pvalue)\n",
    "    p_values_paired_x_z.append(ttest_rel(x, z).pvalue)\n",
    "    p_values_ind_x_k.append(ttest_ind(x, k, equal_var=True).pvalue)\n",
    "    p_values_paired_x_k.append(ttest_rel(x, k).pvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a4e378",
   "metadata": {},
   "source": [
    "3.6 Recall that the power of a binary hypothesis test is the probability that the test correctly rejects the null hypothesis when the alternative hypothesis is true. Using the p-values and assuming that we reject the null hypothesis if $p < 0.05$, calculate the statistical power of:\n",
    "- The independent sample t-test comparing $X$ and $Z$.\n",
    "- The paired t-test comparing $X$ and $Z$.\n",
    "- The independent sample t-test comparing $X$ and $K$.\n",
    "- The paired t-test comparing $X$ and $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc870b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "stat_power_ind_x_z = len([x for x in p_values_ind_x_z if x < 0.05]) / n_iters\n",
    "stat_power_paired_x_z = len([x for x in p_values_paired_x_z if x < 0.05]) / n_iters\n",
    "\n",
    "stat_power_ind_x_k = len([x for x in p_values_ind_x_k if x < 0.05]) / n_iters\n",
    "stat_power_paired_x_k = len([x for x in p_values_paired_x_k if x < 0.05]) / n_iters\n",
    "\n",
    "print(\n",
    "    stat_power_ind_x_z, stat_power_paired_x_z, stat_power_ind_x_k, stat_power_paired_x_k\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6a8055",
   "metadata": {},
   "source": [
    "3.7 **Discuss:** When are paired t-tests helpful? Justify.\n",
    "\n",
    "3.8 With a bootstrapping approach (implemented by yourself, you should not use existing bootstrapping functions), estimate the average difference and 95% confidence intervals for:\n",
    "- the mean ratio between the number of words in the winner headline and the loser headline (i.e., the number of words in the winner headline divided by the number of words in the loser headlines).\n",
    "- the difference in usage of positive words between winner and loser headlines.\n",
    "- the difference in usage of negative words between winner and loser headlines.\n",
    "- The difference in usage of each type of pronoun between winner and loser headlines.\n",
    "\n",
    "3.9 **Discuss:** According to the results obtained in **T3.8**, what headlines grab people's attention the most? Justify your answer.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e8777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confidence_interval(a):\n",
    "    return (np.quantile(a, 0.025), np.quantile(a, 0.975))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575473cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "ci_mean_ratio = confidence_interval(\n",
    "    comparaison_final[\"numwords1\"] / comparaison_final[\"numwords2\"]\n",
    ")\n",
    "# 2\n",
    "ci_pos_words = confidence_interval(\n",
    "    comparaison_final[\"positive1\"] - comparaison_final[\"positive2\"]\n",
    ")\n",
    "avg_diff_pos_words = (\n",
    "    comparaison_final[\"positive1\"] - comparaison_final[\"positive2\"]\n",
    ").mean()\n",
    "# 3\n",
    "ci_neg_words = confidence_interval(\n",
    "    comparaison_final[\"negative1\"] - comparaison_final[\"negative2\"]\n",
    ")\n",
    "avg_diff_neg_words = (\n",
    "    comparaison_final[\"negative1\"] - comparaison_final[\"negative2\"]\n",
    ").mean()\n",
    "# 4\n",
    "ci_prononouns = {}\n",
    "avg_diff_pronouns = {}\n",
    "for key in feature_wordsets:\n",
    "    ci_prononouns[key] = confidence_interval(\n",
    "        comparaison_final[key + \"1\"] - comparaison_final[key + \"2\"]\n",
    "    )\n",
    "    avg_diff_pronouns[key] = (\n",
    "        comparaison_final[key + \"1\"] - comparaison_final[key + \"2\"]\n",
    "    ).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70888d20",
   "metadata": {},
   "source": [
    "## Task 4: Temporal validity and heterogeneity of the effect.\n",
    "\n",
    "Last, we investigate how the effects studied in **T3** change with time and how they might be heterogeneous across different types of news.\n",
    "\n",
    "4.1 Create a plot where you depict the monthly average number of words in winner and loser headlines. Consider only headlines created after April 2013 (the month of April inclusive). Include also bootstrapped 95% confidence intervals; here, you can use a third-party implementation if you want. Finally, recall that we created a column `date_created` which captures the creation of the winner headline; you can consider this date to correspond to the date of the creation of the A/B test.\n",
    "\n",
    "4.2 Produce similar plots to each pronoun category, as well as for positive and negative sentiment. Here, unlike in **T4.1**, depict the month averages pooled across winner and loser headlines (i.e., for each month, you calculate the average across both winners and loser headlines).\n",
    "Create all these plots in a single figure with no more than 11 inches of width and 11 inches of height. Again, consider only headlines created after April 2013 (the month of April inclusive).\n",
    "\n",
    "4.3 **Discuss:** Has the type of headline Upworthy used in their A/B tests changed with time? Are these changes likely to be producing more or less engaging headlines? Justify.\n",
    "\n",
    "4.4 Divide your data into two periods, $t_1$, which goes from  April 2013 (inclusive) to March 2014 (inclusive), and $t_2$, which goes from April 2014 (inclusive) to the latest A/B test in the data. Create a dataframe for A/B tests in each period.\n",
    "\n",
    "4.5 Let's examine if the effects observed remained the same throughout the study period. Use an appropriate methodology  of your choice to determine if the effects observed in **T3.8** (length, each category of pronouns, positive words, and negative words) were different in $t_1$ and $t_2$. Here, note that we are considering \"at least one positive outcome\" to be the manifestation of an underlying effect, thus significance level must be adjusted down when performing multiple hypothesis tests!\n",
    "\n",
    "4.6 **Discuss:** Hypothesize two reasons that could have led to a change in the observed effects. According to the analysis done in **T4.5**, have the effects observed remained the same across the study period? \n",
    "\n",
    "4.7 The features we are studying may interact with each other. For instance, people may like first person singular pronouns in headlines containing positive words (you are amazing!), but dislike headlines with negative words and first person pronouns (you are awful!). To help answer this question, create:\n",
    "- a dataframe containing all A/B tests where both winner and loser headlines include a positive word; and\n",
    "- a dataframe containing all A/B tests where both winner and loser headlines include a negative word;\n",
    "\n",
    "4.8 Using an appropriate methodology of your choice, determine if the effect of the use of first person singular pronouns in the headline is heterogeneous across headlines with positive words and negative words, i.e., is the effect significantly stronger for one of the dataframes created in **T4.7**? \n",
    "\n",
    "4.9 **Discuss:** Considering the analyses you did throughout Tasks 3 and 4, write a short text (no more than 250 words) giving advice to Upworthy employees on how they should try to write engaging headlines. \n",
    "You can reference images present in the notebook by indicating a task (e.g., image plotted in **T3.3**) or a cell number. Note that you do not need to conduct any additional analysis to write this text. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3dc9f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
